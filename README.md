
This system allows you to capture **live video**, **extract meaningful events**, store them in memory, and ask **natural language questions** about anything that has happened â€” just like a human assistant watching a video feed.

## ğŸ“Œ Project Highlights

- ğŸ“¹ Live camera input processing
- ğŸ” Object & scene detection using [CLIP](https://huggingface.co/openai/clip-vit-base-patch32)
- ğŸ§  Video understanding using **Video-LLaVA-style architecture** (multi-modal memory + LLM reasoning)
- ğŸ“š Memory system built on **FAISS** for fast vector search
- ğŸ¤– Natural language QA with Hugging Face's Falcon-7B-Instruct

---

## ğŸ—ï¸ Architecture Overview 

### ğŸ”„ Streaming Pipeline (Video-LLaVA Style)

1. **Live Capture**: Frames streamed in real-time via webcam (OpenCV)
2. **Perceptual Encoding**: Frames converted to feature embeddings via CLIP (mimicking the visual encoder in Video-LLaVA)
3. **Event Abstraction**: Events are inferred from tags (e.g., â€œPerson, cup, phoneâ€ â†’ â€œPerson picked up phoneâ€)
4. **Vector Storage**: Events are embedded (mock or real) and stored in a searchable FAISS vector index
5. **LLM Agent**: Falcon-7B-Instruct (or any Hugging Face LLM) reasons over past events using retrieval + generation

---

## ğŸ”§ Requirements

Install all required dependencies using:

```bash
pip install -r requirements.txt
```

Make sure you have:
- Python 3.8+
- An NVIDIA GPU (recommended for inference)
- Hugging Face model cache (some downloads may take time)

---

## ğŸš€ Running the System

```bash
python main.py
```

The system will:
1. Start capturing frames from webcam
2. Detect objects using CLIP
3. Build a semantic memory of visual events
4. Enter QA mode so you can ask:  
   > "What objects were detected earlier?"  
   > "Did someone pick up a cup?"  
   > "What happened after a person entered?"

---

## ğŸ’¬ Example Workflow

```bash
Ask a question (or type 'exit' to quit): What did the person do?
Answer: Detected: person, phone â€” Possibly picked up phone
```

---

## ğŸ” Improvements over standard  Video-LLaVA architecture

| Component            | This System                        | Video-LLaVA Equivalent          |
|---------------------|------------------------------------|---------------------------------|
| Visual Perception    | CLIP object/scene tagging          | ViT/BLIP2 video encoder         |
| Event Abstraction    | Tag-based event builder            | Multimodal decoder (Q-Former)   |
| Long-term Memory     | FAISS vector store                 | Memory stack in LLaVA           |
| LLM Agent            | Falcon-7B-instruct pipeline        | LLaVA decoder (ChatGPT-4o like) |
| Input Type           | Live stream from webcam            | Video snippets / multi-modal    |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py                  # Entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ video/
â”‚   â”œâ”€â”€ capture.py           # OpenCV-based video stream
â”‚   â””â”€â”€ processor.py         # Frame tagger (CLIP)
â”œâ”€â”€ embedding/
â”‚   â”œâ”€â”€ event_extractor.py   # Tag-to-event abstraction
â”‚   â””â”€â”€ vector_store.py      # FAISS-based memory store
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ query_agent.py       # LLM-based QA over event memory
â”‚   â””â”€â”€ prompts.py           # QA prompt templates
â””â”€â”€ utils/
    â””â”€â”€ logger.py            # Optional logging utility
```

---

## ğŸ› ï¸ Extending This System

we are working on:
- Replacing mock embeddings with **CLIP sentence embeddings**
- Swapping Falcon-7B with **Ollama** or **Mistral-7B** if you want local inference
- Adding FastAPI for a browser-based chat UI
- Incorporating full video scene description models like **VideoBLIP** or **VideoLLaVA**

---

## ğŸ“£ Acknowledgments

- [Video-LLaVA Paper](https://arxiv.org/abs/2403.08016)
- [OpenAI CLIP](https://github.com/openai/CLIP)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [FAISS](https://github.com/facebookresearch/faiss)

---

## ğŸ“¬ Contact

Built by ChatGPT for Kevin Anaba ğŸ’¡. Customizable for surveillance, smart monitoring, or research. Let me know if you want deployment-ready Docker + GUI!
